**1.	Introduction**



Social networking site uses opinion mining has been recently introduced to extract useful information from text and multimedia data. A new real-time method is required to extract the ideas and thoughts from this huge data. Such information can prove useful in many fields, such as economics, politics, media, culture. However, there are a lot of data on social networking sites, technologies that can efficiently process large amounts of unstructured data to obtain the necessary information are very important as fundamental technologies in social media data processing.

Social media is social interaction among people, in which they create, share, or exchange information, ideas, and pictures, videos, texts in virtual communities and networks. Social media contains many forms of multimedia information. Twitter contains various forms of information. In this, we focus on text data and we study methods for extraction of sentiment information from big-data text.

Since the data format is relatively easy and free, most social networking site data are unstructured. Unstructured data may be defined as data that has not been standardized, because its structure and shape are so complex, unlike video image data and document data [1]. In order to extract meaningful information from large amounts of unstructured data on a social networking site, a structuring process is needed for the unstructured data.

Various open sources associated with the processing of big data have been provided. The Hadoop ecosystem is a famous big data processing system that is most commonly used. Hadoop Distributed File System (HDFS) and MapReduce functions based on Hadoop are proposed, which collect and store a variety of data generated by social networks and analyze the sentiments of users on networks that have a lot of unstructured data.
 
**1.1	Motivation**

The work which was used in MuSigma was used to know which topic were discussed on twitter more and in which way. They conducted Sentimental analysis of data from Twitter to know about the trending technology discussed . The result was shown in the figure 1.1:

<img width="759" height="479" alt="image" src="https://github.com/user-attachments/assets/e48dfc95-3d1d-4366-bc32-ea3d4b02f88e" />


                                                    Figure:1.1: Word Cloud Representation for sentimental analysis

This work motivated us to work on this type of project which was utilized by us in predicting the next leader from the sentiments of twitter data.



**1.2	Problem statement:**

In this project, we extract the data from Twitter and perform Sentiment Analysis. Using this we will be predicting the next leader.
 
**1.3	Objective:**

Our aim is to find out the next coming up leader who will be leading us into bright futures. We are doing this by collecting twitter tweets on famous personalities who will be standing for this authority.

Hadoop

Hadoop is an open-source software framework for storing data and running applications on clusters of commodity hardware. It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs[1].

Importance of Hadoop

•	Ability to store and process huge amounts of any kind of data, quickly. With increasing volumes and varieties of data, especially from social media and the Internet of Things (IoT), that's a key consideration.
•	Computing power. The more computing nodes you use, the more processing power you have.
•	Data Distribution. If a node goes down, jobs are automatically redirected to other nodes to make sure the distributed computing does not fail. Multiple copies of all data are stored automatically.
•	Flexibility. Unlike relational databases, you don’t have to preprocess data before storing it. You can store as much data as you want and decide how to use it later. That includes unstructured data like text, images and videos.
•	Low cost. The open-source framework is free and uses commodity hardware to store large quantities of data.
•	Scalability. You can easily grow your system to handle more data simply by adding nodes. Little administration is required.
 
Sentimental Analysis

Sentiment analysis is the process of using text analytics to mine various sources of data for opinions. Often, sentiment analysis is done on the data that is collected from the Internet and from various social media platforms. Politicians and governments often use sentiment analysis to understand how the people feel about themselves and their policies.

Data is captured from different sources, such as mobile devices and web browsers, and it is stored in various data formats. Because the social media content is unstructured RDBMS, Relational Database Management System, we need tools that can process and analyze this data. However, big data technology is made to handle the different sources and different formats of the structured and unstructured data.

Every day massive amount of data is generated by social media users which can be used to analyze their opinion about any event, movie, product or politics. Conventional tools like Apache Storm analyze stream in micro-batch whereas novel tools like Apache Spark process data in real time making analyzing and processing of real time data possible.
 
**2.	Literature Survey**



[1]	Rodolfo Ferro (2017-18) Sentiment analysis on Trump’s tweets using Python DEV Community. This survey is based on a workshop that author has taught in Mexico City[2]. In this he has shown simple ways of representing the most number of retweeted tweets and more likes, extracting twitter data using tweepy and learn how to handle it using pandas. For extraction of tweets the author has used the following function: extractor.user_timeline(screen_name, count) to extract from screen_name. Doing some basic statistics and visualizations with numpy, matplotlib and seaborn. This is done by collecting the tweet with more likes and more retweets. What is done here is that, the maximum number of likes from the 'Likes'column and the maximum number of retweets from the 'RTs'usingnumpy's max function. With this the author looks for the index in each of both columns that satisfy to be the maximum. Doing sentiment analysis of extracted (Trump's) tweets using textblob. The author uses the re library from Python, which is used to work with regular expressions. Author has represented the output as a graph marking the values of retweets and likes about Trump on particular dates.



[2]	Marco Bonzanini (2015) Mining Twitter Data with Python (Part 1: Collecting data). This article is dedicated to mining data on Twitter using Python. In this article we will see different options to collect data from Twitter[3]. Firstly, he has registered for app that interacts with the Twitter API and register a new application. Twitter provides REST APIs that are used to interact with their service. Here author has used tweepy which is one of the most interesting and straightforward platform to use for data collection. The collected data is converted in the json format. To “keep the connection open”, and gather all the upcoming tweets about a particular event, author used the streaming API. Author has introduced tweepy as a tool to access Twitter data in a fairly easy way with Python.
 
[3]	PRATEEK JOSHI (2018) Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code. In this article author has described the following as a sequence of steps needed to solve a general sentiment analysis problem for finding out the words associated with racist or sexist tweets, understand the Problem Statement. The task author has chosen is to classify racist or sexist tweets from other tweets. Tweets Preprocessing and Cleaning. Preprocessing of the text data is an essential step as it makes the raw text ready for mining, as it becomes easier author has extracted information from the text and applied machine learning algorithms. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don’t carry much weightage in context to the text. Story Generation and Visualization from Tweets. Data visualization is done using Word Cloud and also a bar graph represents the word count. From this article, we have learned how to approach a sentiment analysis problem. Author started with preprocessing and exploration of data. Ended it with data visualization.

[4]	Machine Learning-Based Sentiment Analysis for Twitter Accounts. Department for Management of Science and Technology Development, Ton Duc Thang University, Ho Chi Minh City, Vietnam. The contribution of this paper includes the adoption of a hybrid approach that involves a sentiment analyzer that includes machine learning. Moreover, this paper also provides a comparison of techniques of sentiment analysis in the analysis of political views by applying supervised machine-learning algorithms such as Naïve Bayes[5]. This framework explained a process from the collection, sentiment analysis, and classification of Twitter opinions. This paper focuses on the adoption of various sentiment analyzers with machine-learning algorithms to determine the approach with the highest accuracy rate for learning about election sentiments. The comparison that has adopted a hybrid approach for sentiment analysis we have learned that TextBlob and Wordnet use word sense disambiguation with greater accuracies and can be used further in predicting elections.
 
[5]	An Introduction to Text Mining using Twitter Streaming API and Python. In this tutorial, the author used Twitter data to compare the popularity of 3 programming languages: Python, Javascript and Ruby. He explained the connection of Twitter Streaming API[6] to get the data. Explanation for structure of data for analysis is provided. Filtration of data . Mining the tweets:

-Adding Python, Ruby, and Javascript tag

-Targeting relevant tweets

The author used lambda function to search for relevant data from the keywords. The tweets DataFrame contains information about all tweets that contains one of the 3 keywords.

For piping the output to a file using the following command: Python_streaming.py>twitter_data.txt .

The author used 4 Python libraries. Json for parsing the data, pandas for data manipulation, matplotlib for creating charts, and re for regular expressions. In this tutorial, we covered many techniques used in text mining. Here the ruby showed the highest popularity.
 
**3.	System Analysis & Design**



**3.1	Requirement Analysis**

"Naive Bayes Classification" is termed such because it makes the assumption that each word is statistically independent from each other word. Sentiment analysis feels quite a bit like classification: given a document, would you label it "positive" or "negative"? The main difference between classification and sentiment analysis is the idea that classification is objective but sentiment is subjective. But let's make another "naive" assumption and pretend that we don't care about that. Let's try applying Bayes to sentiment and see what happens.

Our Bayes classifier can easily be extended to use tokenizing our document two words at a time instead of one! Rather than treating the words "This" "movie" "was" "not" "great" separately --and risk getting confused by the negation -- all we have to do is record them as pairs: "This movie" "movie was" "was not" "not great".

Now our classifer knows the difference between "was great" and "not great"! In a text classification problem, we will use the words (or terms/tokens) of the document in order to classify it on the appropriate class. By using the “maximum a posteriori (MAP)” decision rule, we come up with the following classifier:


<img width="423" height="86" alt="image" src="https://github.com/user-attachments/assets/d1e04d7f-db79-4118-913b-02c9a8a810ce" />


Where tk are the tokens (terms/words) of the document, C is the set of classes that is used in the classification, the conditional probability of class c given document d, the prior probability of class c and the conditional probability of token tk given class c.
 
This means that in order to find in which class we should classify a new document, we must estimate the product of the probability of each word of the document given a particular class (likelihood), multiplied by the probability of the particular class (prior). After calculating the above for all the classes of set C, we select the one with the highest probability.

**3.2	Specifications:**

Hardware requirements:

Some of the essential hardware requirements for installing Hadoop are:

•	Intel Core 2 Duo/Quad/hex/Octa or higher end 64 bit processor PC or Laptop (Minimum operating frequency of 2.5GHz)
•	Hard Disk capacity of 1- 4TB.
•	16-512 GB RAM
•	10 Gigabit Ethernet or Bonded Gigabit Ethernet Software requirements:

	VMware Workstation 12 Player
	Ubuntu version-17.04 for hadoop version-2.0
	Python version-3.0.4
	Tweepy
	Pandas
	Numpy
	Textblob
	Matplotlib
 
**3.3	UML DIAGRAMS**

Use case diagram

Use Cases. A use case is a written description of how users will perform tasks on your website. It outlines, from a user's point of view, a system's behavior as it responds to a request. Each use case is represented as a sequence of simple steps, beginning with a user's goal and ending when that goal is fulfilled.

<img width="836" height="709" alt="image" src="https://github.com/user-attachments/assets/106bc13d-4df2-4a38-a17e-2a407e77990f" />

                                                                    Figure 3.1 Use Case Diagram
 

Our project uses figure 3.1 consisting of user, twitter admin, user interface .User can access the twitter API upon authorization, he can fetch tweets and performs sentimental analysis and the results are shown in the form of graphs, pie charts etc. Twitter authorization requires http request to twitter and http response is sent to particular user after authorization from Twitter. The output is shown in such a way which is easily understandable i.e. user- interface.

Sequence diagram

A sequence diagram simply depicts interaction between objects in a sequential order i.e. the order in which these interactions take place. We can also use the terms event diagrams or event scenarios to refer to a sequence diagram. Sequence diagrams describe how and in what order the objects in a system function.

<img width="884" height="570" alt="image" src="https://github.com/user-attachments/assets/b6be569c-f935-4e4b-901f-597a70c05dfe" />

                                                                        Figure 3.2 Sequence Diagram
 
The sequence diagram figure 3.2 shows the sequence in which first the reviews are taken from the testing data and sent to the next scenario which is feature extraction .After the feature is extracted , these are sent to the classifier for getting sentiment.

Activity diagram

Activity diagram is another important diagram in UML to describe the dynamic aspects of the system. Activity diagram is basically a flowchart to represent the flow from one activity to another activity. The activity can be described as an operation of the system. The control flow is drawn from one operation to another.

<img width="662" height="788" alt="image" src="https://github.com/user-attachments/assets/ac36c8bb-2793-421f-bb9c-753e082d4b6f" />


                                                                  Figure 3.3 Activity Diagram
 
The activity diagram in figure 3.3 of our project represents a flow in which the process is carried out.First data is extracted from twitter about the political leaders and then each review is stored in database and from these reviews ,features are extracted .These are given to the Naive Bayes classifier for classification and display the results in either bar graph or pie chart.

System flow diagram

A system flow diagram is a way to show relationships between a business and its components, such as customers (according to IT Toolbox.)System flow diagrams, also known as process flow diagrams or data flow diagrams, are cousins to common flowcharts.

<img width="535" height="612" alt="image" src="https://github.com/user-attachments/assets/9c883e7a-74b0-4155-a726-79dc4b5b9fe3" />

                                                                            Figure:3.4 Flow diagram
 
The system diagram in figure 3.4 and 3.5 of our project shows first the data related to the given keywords (political leaders in our project) is retrieved from twitter and this data is processed to get the feature. The features extracted are given to the Naive Bayes classifier for classification. After classification the scores of each political party are aggregated and the final result is shown in pie chart.

<img width="540" height="675" alt="image" src="https://github.com/user-attachments/assets/e2b691e1-b085-469b-8857-cae36d56b41a" />

                                                                          Figure 3.5 System Flow Diagram

Data flow diagram

A data flow diagram is a graphical representation of the "flow" of data through an information system, modelling its process aspects. A DFD is often used as a preliminary step to create an overview of the system without going into great detail, which can later be elaborated.
 
<img width="388" height="431" alt="image" src="https://github.com/user-attachments/assets/4f90193b-7c07-4960-9072-eb6b0e7335b0" />

                                                                Figure 3.6 Data Flow Diagram
 
The data flow diagram in figure 3.6 of our project is the preliminary step which gives an overview of Tweet retrieval and Processing data,feature extraction and then passing them to the Naive Bayes classifier and calculating the polarity of each sentiment.
 
**4.	Methodology**

**4.1	System architecture**


<img width="749" height="961" alt="image" src="https://github.com/user-attachments/assets/07b7fdbc-32b5-4d44-9381-263b78812f74" />

Figure:4.1 System architecture
 

Social networking site uses opinion mining has been recently introduced to extract useful information from text and multimedia data. A new real-time method is required to extract the ideas and thoughts from this huge data. Such information can prove useful in many fields, such as economics, politics, media, culture. However, there are a lot of data on social networking sites, technologies that can efficiently process large amounts of unstructured data to obtain the necessary information are very important as fundamental technologies in social media data flows as shown in the figure 4.1.

Social media is social interaction among people, in which they create, share, or exchange information, ideas, and pictures, videos, texts in virtual communities and networks. Social media contains many forms of multimedia information. Twitter contains various forms of information, such as video links, image links, and text data. In this, we focus on text data and we study methods for extraction of sentiment information from big-data text.

Since the data format is relatively easy and free, most social networking site data are unstructured. Unstructured data may be defined as data that has not been standardized, because its structure and shape are so complex, unlike video image data and document data. In order to extract meaningful information from large amounts of unstructured data on a social networking site, a structuring process is needed for the unstructured data. Up to now, various technologies for processing unstructured data have been studied, focusing on morphological analysis.

Various open sources associated with the processing of big data have been provided. The Hadoop ecosystem is a famous big data processing system that is most commonly used. Hadoop Distributed File System (HDFS) and MapReduce functions based on Hadoop are proposed, which collect and store a variety of data generated by social networks and analyze the sentiments of users on networks that have a lot of unstructured data.
 
Steps to do Sentiment Analysis:

Data Collection:
We have to get the following API keys for data collection 
consumer_key<-„xxxxxxxx’
consumer_secret<-'xxxxxxxxxxx’ 
access_token<-'xxxxxxxxxxxx’ 
access_secret<-„xxxxxxxxxxx’

This is done by creating a developer account in twitter as shown in the figure 4.2

<img width="877" height="221" alt="image" src="https://github.com/user-attachments/assets/7a021ba9-5e5a-48c0-b760-9ba8720ba142" />

                                          Figure: 4.2 Command for storing the collected data into a text file directly

Data Processing:

Data processing involves Tokenization which is the process of splitting the tweets into individual words called tokens. Tokens can be split using whitespace or punctuation characters. It can be unigram or bigram depending on the classification model used. The bag-of-words model is one of the most extensively used model for classification. It is based on the fact of assuming text to be classified as a bag or collection of individual words with no link or interdependence. The simplest way to incorporate this model in our project is by using unigrams as features. It is just a collection of individual words in the text to be classified, so, we split each tweet using whitespace as shown in the figure 4.3.
 

 <img width="989" height="465" alt="image" src="https://github.com/user-attachments/assets/a7f2cb1a-91bb-4689-9111-50064ed61895" />


                                                        Figure 4.3 : Data collection in a text file .

Data Filtering:

A tweet acquired after data processing still has a portion of raw information in it which we may or may not find useful for our application. Thus, these tweets are further filtered by removing stop words, numbers and punctuations.

We will be gathering data from multiple sources in multiple formats (structured, semi-structured, or unstructured), you need to consider setting up a Hadoop cluster and a Hadoop Distributed File System (HDFS) to store your data shown in the figure 4.4. An HDFS provides a flexible way of managing big data:

-We move some of your analyzed data into an existing relational database management system(RDBMS).

-We store the data in HDFS for future analysis, such as comparing old data with new data.

-We discard the data if you just need the analysis of the data at the point of impact.
 

 <img width="917" height="490" alt="image" src="https://github.com/user-attachments/assets/57f7a3a7-dd86-445f-ba80-1344a0f09c8b" />


                                                        Figure 4.4: Filtering of tweets from the collected data.

Data Extraction (Retrieving data from a Twitter feed):

Twitter, a popular blogging site, has a set of APIs that allows us to retrieve and manipulate tweets. First, however, we need to implement Twitter's OAuth framework. In simple terms, with this framework, an application can log in to Twitter on your behalf without you needing to log in on the Twitter website. Review the setup process on Twitter's developer site that explains how to designate the application that does this. In this process, you are assigned a key and a secret token that your application uses to authenticate on your behalf. After your application is authenticated, you can then use the Twitter APIs to fetch tweets shown in the figure 4.5.

The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 40 petabytes of enterprise data at Yahoo.
 
Features:

•	Rack awareness allows consideration of a node’s physical location, when allocating storage and scheduling tasks.
•	Minimal data motion. MapReduce moves compute processes to the data on HDFS and not the other way around. Processing tasks can occur on the physical node where the data resides. This significantly reduces the network I/O patterns and keeps most of the I/O on the local disk or within the same rack and provides very high aggregate read/write bandwidth.
•	Utilities diagnose the health of the files system and can rebalance the data on different nodes.
•	Rollback allows system operators to bring back the previous version of HDFS after an upgrade, in case of human or system errors.
•	Standby NameNode provides redundancy and supports high availability.
•	Highly operable. Hadoop handles different types of cluster that might otherwise require operator intervention. This design allows a single operator to maintain a cluster of 1000s of nodes.


<img width="747" height="218" alt="image" src="https://github.com/user-attachments/assets/bd7e403c-8008-4fb5-966d-998ac7d3647c" />


                                                        Figure4.5: Data extraction This figure represents the commands:wer
                                                        (i)	for changing the directory (“cd”)
                                                        (ii)	for running the python file for classifying the processed data
 
**4.2	Sentiment Analysis:**

Performing sentiment analysis on data:

Now that we combined the data, we can complete the sentiment analysis on a single data source, which allows for uniformity, consistency, and accuracy of our analyses. You can use Pig or Hive to do these analyses. Pig and Hive are SQL-like syntax languages that run on the Hadoop platform.

To complete sentiment analyses, we need to have a dictionary of words or word list. The dictionary includes a set of standard words that depicts positive and negative words within a context. It identifies sarcasm words, innuendos, slang terms, new vocabulary, characters, and smileys that are often used in social media.

To handle a variety of unstructured social networking site (SNS) data efficiently, a big data processing system is proposed[5]. The proposed system is comprised of a parallel HDFS and MapReduce. HDFS, which is the Hadoop ecosystem, is used to collect and store data from a large amount of SNS data. MapReduce is used to effectively analyze large amounts of unstructured data as to the sentiment of the user.

Sentiment analysis is the process of using text analytics to mine various sources of data for opinions. Often, sentiment analysis is done on the data that is collected from the Internet and from various social media platforms. Politicians and governments often use sentiment analysis to understand how the people feel about themselves and their policies.

With the advent of social media, data is captured from different sources, such as mobile devices and web browsers, and it is stored in various data formats. Because the social media content is unstructured with respect to traditional storage systems (such as RDBMS, Relational Database Management System), we need tools that can process and analyze this disparate data. However, big data technology is made to handle the different sources and different formats of the structured and unstructured data.
 

<img width="897" height="505" alt="image" src="https://github.com/user-attachments/assets/b15f3395-824d-49c5-b2ba-a82bc6b03a9c" />

                                                                        Figure 4.6 HDFS Architecture

The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case shown in the figure 4.6. The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.

HDFS is a file processing system that has a distributed processing structure. It is configured in parallel, as shown in the above Figure and each chunk node for storing data is set to 64 MB. It duplicates the name server using NFS for disaster recovery.
 
**4.3	Functionality**

VMware Workstation Player

It (formerly known as Player Pro) is a desktop virtualization application that is available for free for personal use. A Commercial License can be applied to enable Workstation Player to run Restricted Virtual Machines created by VMware Workstation Pro and Fusion Pro.

VMware Workstation Player installs like a standard desktop application. Once installed, VMware Workstation Player allows you to install new operating systems and run them as virtual machines in a separate window. VMware Workstation Player includes features that enable users to create and configure their own virtual machines for optimal performance and access any devices connected to their PC.

Use VMware Workstation Player to create, run, evaluate, and share software running in virtual machines:

•	Create: Use VMware Workstation Player to create virtual machines with the latest 32-bit and 64- bit Windows and Linux operating systems. With Easy Install it’s easier than installing them directly on your PC.
•	Run: VMware Workstation Player can be used by anyone to run virtual machines on a Windows or Linux PC. VMware Workstation Player makes it quick and easy to take advantage of the security, flexibility, and portability of virtual machines.
•	Evaluate: VMware Workstation Player is ideal for safely evaluating software distributed as a virtual appliance. Virtual appliances are pre-built, pre-configured and ready-to-run enterprise software applications packaged along with an operating system in a virtual machine. With VMware Workstation Player, anyone can quickly and easily experience the benefits of preconfigured products without any installation or configuration hassles. Run over 900 virtual appliances from leading software vendors.

VMware Workstation Player enables you to quickly and easily create and run virtual machines. The Workstation Player user interface is designed to be as easy to use as possible. It is intended
 
for people who need to run virtual machines, typically provided to them by their IT organization, system administrator, instructor, software supplier etc.

VMware Workstation Pro is much more advanced and comes with powerful features including snapshots, cloning, remote connections to vSphere or vCloud Air, sharing VMs, advanced Virtual Machines settings and much more. Workstation is designed to be used by technical professionals such as developers, quality assurance engineers, systems engineers, IT administrators, technical support representatives, trainers, etc.

Python:

Python is a high-level, interpreted, interactive and object-oriented scripting language. Python is designed to be highly readable. It uses English keywords frequently where as other languages use punctuation, and it has fewer syntactical constructions than other languages.

	Python is Interpreted − Python is processed at runtime by the interpreter. You do not need to compile your program before executing it. This is similar to PERL and PHP.
	Python is Interactive − You can actually sit at a Python prompt and interact with the interpreter directly to write your programs.
	Python is Object-Oriented − Python supports Object-Oriented style or technique of programming that encapsulates code within objects.
	Python is a Beginner's Language − Python is a great language for the beginner-level programmers and supports the development of a wide range of applications from simple text processing to WWW browsers to games.

Python Features

Python's features include −

•	Easy-to-learn − Python has few keywords, simple structure, and a clearly defined syntax. This allows the student to pick up the language quickly.
•	Easy-to-read − Python code is more clearly defined and visible to the eyes.
•	Easy-to-maintain − Python's source code is fairly easy-to-maintain.
 
•	A broad standard library − Python's bulk of the library is very portable and cross- platform compatible on UNIX, Windows, and Macintosh.
•	Interactive Mode − Python has support for an interactive mode which allows interactive testing and debugging of snippets of code.
•	Portable − Python can run on a wide variety of hardware platforms and has the same interface on all platforms.
•	Extendable − You can add low-level modules to the Python interpreter. These modules enable programmers to add to or customize their tools to be more efficient.
•	Databases − Python provides interfaces to all major commercial databases.
•	GUI Programming − Python supports GUI applications that can be created and ported to many system calls, libraries and windows systems, such as Windows MFC, Macintosh, and the X Window system of Unix.

Tweepy

If you are new to Tweepy, this is the place to begin. The goal of this tutorial is to get you set-up and rolling with Tweepy.

Hello Tweepy Importtweepy

auth=tweepy.OAuthHandler(consumer_key,consumer_secret)
auth.set_access_token(access_token,access_token_secret) api=tweepy.API(auth)

public_tweets=api.home_timeline() fortweetinpublic_tweets: printtweet.text

This example will download your home timeline tweets and print each one of their texts to the console. Twitter requires all requests to use OAuth for authentication. The Authentication Tutorialgoes into more details about authentication.

API

The API class provides access to the entire twitter RESTful API methods. Each method can accept various parameters and return responses. For more information about these methods please
 
refer to API Reference.

Models

When we invoke an API method most of the time returned back to us will be a Tweepy model class instance. This will contain the data returned from Twitter which we can then use inside our application. For example the following code returns to us an User model:

OAuth Authentication

Tweepy tries to make OAuth as painless as possible for you. To begin the process we need to register our client application with Twitter. Create a new application and once you are done you should have your consumer token and secret. Keep these two handy, you’ll need them.

The next step is creating an OAuthHandler instance. Into this we pass our consumer token and secret which was given to us in the previous paragraph:

auth=tweepy.OAuthHandler(consumer_token,consumer_secret)

If you have a web application and are using a callback URL that needs to be supplied dynamically you would pass it in like so:

auth=tweepy.OAuthHandler(consumer_token,consumer_secret, callback_url)

If the callback URL will not be changing, it is best to just configure it statically on twitter.com when setting up your application’s profile.

Unlike basic auth, we must do the OAuth “dance” before we can start using the API. We must complete the following steps:
1.	Get a request token from twitter
2.	Redirect user to twitter.com to authorize our application
3.	If using a callback, twitter will redirect the user to us. Otherwise the user must manually supply us with the verifier code.
 
4.	Exchange the authorized request token for an access token.

Pandas

Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.

pandas isaNUMFOCUS sponsored project. This will help ensure the success of development of pandas as a world-class open-source project, and makes it possible to donate to the project.

Python has long been great for data munging and preparation, but less so for data analysis and modeling. pandas helps fill this gap, enabling you to carry out your entire data analysis workflow in Python without having to switch to a more domain specific language like R. Combined with the excellent Ipython toolkit and other libraries, the environment for doing data analysis in Python excels in performance, productivity, and the ability to collaborate. Pandas does not implement significant modeling functionality outside of linear and panel. More work is still needed to make Python a first class statistical modeling environment, but we are well on our way toward that goal.

TextBlob

It is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.

Features

•	Noun phrase extraction
•	Part-of-speech tagging
•	Sentiment analysis
•	Classification (Naive Bayes, Decision Tree)
•	Language translation and detection powered by Google Translate
•	Tokenization (splitting text into words and sentences)
•	Word and phrase frequencies
 
•	Parsing
•	n-grams
•	Word inflection (pluralization and singularization) and lemmatization NumPy in Python

NumPy is a general-purpose array-processing package. It provides a high-performance multidimensional array object, and tools for working with these arrays.

It is the fundamental package for scientific computing with Python. It contains various features including these important ones:

•	A powerful N-dimensional array object
•	Sophisticated (broadcasting) functions
•	Tools for integrating C/C++ and Fortran code
•	Useful linear algebra, Fourier transform, and random number capabilities

Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional containerofgenericdata.Arbitrary data-types can be defined using Numpy which allows NumPy to seamlessly and speedily integrate with a wide variety of databases.

Matplotlib

It is an amazing visualization library in Python for 2D plots of arrays. Matplotlib is a multi-platform data visualization library built on NumPy arrays and designed to work with the broader SciPy stack. It was introduced by John Hunter in the year 2002.

One of the greatest benefits of visualization is that it allows us visual access to huge amounts of data in easily digestible visuals. Matplotlib consists of several plots like line, bar, scatter, histogram etc.

Matplotlib comes with a wide variety of plots. Plots helps to understand trends, patterns, and to make correlations. They’re typically instruments for reasoning about quantitative information. Some of the sample plots are covered here.
 
Matplotlib.pyplot is a plotting library used for 2D graphics in python programming language. It can be used in python scripts, shell, web application servers and other graphical user interface toolkits.

There are several toolkits which are available that extend python matplotlib functionality. Some of them are separate downloads, others can be shipped with the matplotlib source code but have external dependencies.

•	Basemap: It is a map plotting toolkit with various map projections, coastlines and political boundaries.
•	Cartopy: It is a mapping library featuring object-oriented map projection definitions, and arbitrary point, line, polygon and image transformation capabilities.
•	Excel tools: Matplotlib provides utilities for exchanging data with Microsoft Excel.
•	Mplot3d: It is used for 3-D plots.
•	Natgrid: It is an interface to the natgrid library for irregular gridding of the spaced data.

Python Matplotlib : Pie Chart

A pie chart refers to a circular graph which is broken down into segments i.e. slices of pie. It is basically used to show the percentage or proportional data where each slice of pie represents a category.Let’s have a look at the example:

importmatplotlib.pyplot as plt days =[1,2,3,4,5] sleeping =[7,8,6,11,7]
eating =[2,3,4,3,2]
working =[7,8,7,2,2]
playing =[8,5,7,8,13]
slices =[7,2,2,13]
activities =['sleeping','eating','working','playing'] cols =['c','m','r','b'] plt.pie(slices,labels=activities,colors=cols,startangle=90,shadow=True,explode=(0,0.1,0,0),autop ct='%1.1f%%')
plt.title('Pie Plot') plt.show()
 
MapReduce:

MapReduce is a software framework developed by Google to support distributed computing, and it allows parallel programming using the function concept called Map. In this paper, it is classified into four special Map functions. They perform polarity preprocessing analysis, syntactic word analysis, morpheme analysis, and prohibitive word analysis, stage by stage. Figure below shows the process of sentiment analysis using four sentiment analysis functions.

Sentiment analysis is started from the loading of sentiment analysis dictionaries[6]. Then, the data are input and loaded to the parallel HDFS. Later, the stepwise sentiment analysis is processed by four sentiment analysis functions. Sentences that do not determine the sensitivity in the previous step move on to the next step. The results of the final sentiment analysis are stored in the database shown in the figure 4.7.


<img width="871" height="552" alt="image" src="https://github.com/user-attachments/assets/c8f3e15f-4ce1-4f44-93a6-949b67025d59" />

                                                              Figure 4.7 Example for MapReduce
 
Errors:

1.	”The virtual machine appears to be in use” error[7]
2.	“pip install unroll:” “python setup.py egg_info” failed with error code 1[8]
3.	Textblob error and Matplotlib error[9]

Error recovery:

1.	For the virtual machine error, the solution is close VMware Workstation[7] delete any

.lck or .lock files and/or folders you see, in the directory of the problematic VM run VMWare Workstation

start the VM

2.	“pip install unroll” error resolved using[8], pip install --upgrade setuptools

If it's already up to date, check that the module ez_setup is not missing. If it is, then pip install ez_setup

3.	The textblob and matplotlib errors are resolved after installing the python 2.7 and 3 versions.[9]
 
Coding:


<img width="715" height="521" alt="image" src="https://github.com/user-attachments/assets/4c5bc6f3-dfcf-457a-97cb-ca6f793f4426" />


                                                            Figure4.8: Data Collection-1 
                                                            
In this figure, we access the Twitter API using
-access_token,
-access_token_secret
-consumer_key
-consumer_secret

We import various methods from tweepy library which are used to collect the data likeStreamListener,OAuthHandler,Stream.


<img width="795" height="435" alt="image" src="https://github.com/user-attachments/assets/6d0df506-69ec-4d88-a025-e5964d1d7b89" />

                                                  Figure4.9: Data filtration by the keywords


<img width="763" height="499" alt="image" src="https://github.com/user-attachments/assets/2ed4974a-96e0-42d6-9e32-52073817e6a1" />

                                                      Figure:4.10: Sentimental analysis
 
We import various methods like json,pandas,numpy,matplotlib,textblob to perform the sentimental analysis and show the output in an efficient way.


<img width="855" height="435" alt="image" src="https://github.com/user-attachments/assets/b4cc4f18-19a1-46de-b1a7-474cd65bc5b5" />

                                                  Figure4.11: Extraction of data from different files

The collected data from different files is taken and the textblob objects are created for calculating the sentiment.


<img width="785" height="385" alt="image" src="https://github.com/user-attachments/assets/47e7cd6a-9ffb-40c9-9473-0a36ed99306d" />

                                              Figure4.12: Classification of positive and negative tweets
 
We use the method named”polarity” to classify the tweets into positive and negative tweets. So we got the positive ,neutral and negative tweets.

<img width="884" height="434" alt="image" src="https://github.com/user-attachments/assets/dc9e1b5a-7e35-4318-bd1d-af81c2344319" />


                                      Figure4.13: Number of positive,neutral and negative tweets for each party

Here ,thepositive,neutral and negative tweets are calculated and printed .Next the percentage is calculated for each party.

<img width="856" height="421" alt="image" src="https://github.com/user-attachments/assets/44fc0f2a-da92-4a1b-a488-9d295ae7eae9" />

                                                      Figure4.14: Representation of output in pie chart
 
For each party, the sentiment is shown in the form of pie chart.Pie chart consists of percent of positive ,neutral,negative tweets of each party.

<img width="883" height="434" alt="image" src="https://github.com/user-attachments/assets/ebe3253c-55a6-4120-922e-a833dd87a71c" />

                                                                      Figure4.15: Final output

The final output is shown of our next leader who has the highest positive opinion among the people.
 
5.	Results


<img width="443" height="398" alt="image" src="https://github.com/user-attachments/assets/5180d56e-104f-459c-9949-7361f46a8785" />


      Figure 5.1.1 This figure represents the percentage of positive, negative and neutral tweets on BJP (from the data collected up to February 28, 2019)

<img width="420" height="379" alt="image" src="https://github.com/user-attachments/assets/187418a7-1aa8-4a86-ac11-d92d087231f8" />


        Figure 5.1.2 This figure represents the percentage of positive, negative and neutral tweets on TDP (from the data collected up to February 28, 2019)

<img width="457" height="351" alt="image" src="https://github.com/user-attachments/assets/e6af997e-f568-4c40-8c01-fe806e2928d3" />


    Figure 5.1.3 This figure represents the percentage of positive, negative and neutral tweets on YSRCP (from the data collected up to February 28, 2019)


<img width="435" height="390" alt="image" src="https://github.com/user-attachments/assets/31d0aab3-4d81-4e38-9833-addf6688fe48" />

        Figure 5.1.4 This figure represents the percentage of positive, negative and neutral tweets on JSP (from the data collected up to February 28, 2019)


 <img width="478" height="359" alt="image" src="https://github.com/user-attachments/assets/a46429b1-2871-4ffb-a834-94d9026d5edc" />

                              Figure 5.1.5 According to February 28, 2019 the predicted next leader is TDP


<img width="608" height="413" alt="image" src="https://github.com/user-attachments/assets/3042d8fc-e17d-434f-a433-0d05c95b0646" />

                              Figure 5.2 According to February 14, 2019 the predicted next leader is YSRCP


<img width="465" height="368" alt="image" src="https://github.com/user-attachments/assets/d73275cb-fa08-4cd2-9582-0d86d7c38acc" />

                                Figure 5.3 According to March 7, 2019 the predicted next leader is JSP


<img width="461" height="416" alt="image" src="https://github.com/user-attachments/assets/23a5224c-3c1d-4009-a1bf-34267c31b9a0" />

        Figure 5.4.1 This figure represents the percentage of positive, negative and neutral tweets on BJP (from the data collected up to March 11, 2019)
 

 <img width="464" height="376" alt="image" src="https://github.com/user-attachments/assets/a60387db-d2f4-44b4-ada5-07c25aca1186" />

        Figure 5.4.2 This figure represents the percentage of positive, negative and neutral tweets on TDP (from the data collected up to March 11, 2019)


<img width="479" height="437" alt="image" src="https://github.com/user-attachments/assets/b7bd1489-d2f5-4ace-9f6b-354d4444c561" />

          Figure 5.4.3 This figure represents the percentage of positive, negative and neutral tweets on JSP (from the data collected up to March 11, 2019)
 

 <img width="472" height="376" alt="image" src="https://github.com/user-attachments/assets/30e2cc25-8ea9-474c-8777-ff6bc8e36c5f" />

          Figure 5.4.4 This figure represents the percentage of positive, negative and neutral tweets on YSRP (from the data collected up to March 11, 2019)


<img width="593" height="439" alt="image" src="https://github.com/user-attachments/assets/5a794a54-f2a8-4e02-8663-4bc643e3552c" />

                                    Figure 5.4.5 According to March 11, 2019 the predicted next leader is YSRCP
 
**6.	Conclusion**



The sentiment analysis functions have processed the data effectively.

The results of sentiment analysis with the proposed system are more effective than the results of sentiment analysis with a manual process.

In this project, we have predicted the next leader using sentimental analysis and represented in the form of pie chart using matplotlib(data visualization).The data has been collected using the API keys given from the Twitter developer account.The data processing and filtering is done by taking the list of leader’s and their parties who are competing in the elections. These lists have been checked for in the twitter data that we have collected using the API keys.

The data has been extracted using the Textblob. The classification of data is done based on Naïve’s Bayes algorithm. Hence the sentiment is obtained on each leader . The polarity is checked for and accordingly predicted the next leader.

Note: Polarity is checked as follows; If polarity>0, tweet is positive. If polarity=0.tweet is neutral. If polarity<0,tweet is negative.
Sentiment analysis and opinion mining have many applications ranging from ecommerce, marketing, to politics and any other research.
 

 <img width="900" height="315" alt="image" src="https://github.com/user-attachments/assets/ac127f4a-348c-4f54-8668-43e01f2a7c03" />

                                        Figure 6.1: The number of positive, negative and neutral tweets on each party

•	Politics: There have been at least a few academic papers examining sentiment analysis in relation to politics[10].
•	Prediction of Indian election on the basis of twitter sentiment analysis.
•	Political data science : Analyzing Trump, Clinton and Sanders Tweets and Sentiment. To sum up, from the marketing standpoint, sentiment analysis helps with:
•	Identifying negative mentions about a business, a service, a company, a marketing campaign, an event in social media and on the Web.
•	Spotting angry customers on the verge of starting a social media crisis Analyzing how your customers react to product changes.
•	Spotting super happy users who, for example, are more likely to become your brand ambassadors.

So, in this way our project, “Finding Next Leader by doing Sentimental Analysis using Hadoop” showed accurate results to bring out the next prominent leader from the public opinion through social network i.e. Twitter.
 
**References**



[1]	Copyright © SAS Institute Inc., SAS Campus Drive, Cary, North Carolina 27513, USA. All rights reserved. Revised Oct. 18, 2018.
[2]	Rodolfo Ferro,“Sentiment analysis on Trump’s tweets using Python”, DEV Community, 2017-18.
[3]	Marco Bonzanini,“ Mining Twitter Data with Python (Part 1: Collecting data)”, 2015.
[4]	Prateek Joshi, “Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code” ,JULY 30, 2018.
[5]	Medhat,	W.;	Hassan,	A.;	Korashy,	H.	“Sentiment	analysisalgorithms	and applications: A survey”. Ain Shams Eng. J. 2014, 5, 1093–1113.
[6]	Adilmoujahid,“An Introduction to Text Mining using Twitter Streaming,API and Python.”, Published July 2014.
[7]	Copyright © 2011-2019 TinkerTry.com, LLC all rights reserved.
[8]	Davis Herring, “https://stackoverflow.com/questions/35991403/pip-install-unroll-python-setup-py-egg-info-failed-with-error-code-1”, July 17,2018.
[9]	Arun Mathew Kurian, “How to build a Twitter sentiment analyzer in Python using TextBlob”,Oct 2014.
[10]	Aayushi	Johari,	”Matplotlib	Tutorial	–	Python	Matplotlib	Library with Examples”, Published on Feb 25,2019.
